{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6814a32",
   "metadata": {},
   "source": [
    "# For the Random Forest Algorithm\n",
    "    1) Number of trees(n_estimators):\n",
    "        after using gridsearch across multiple hyperparameters' values I found that the best number of trees is around 400.\n",
    "\n",
    "    2) Tree Depth (max_depth):\n",
    "        The most promising depth was around 4-7 and I chose 6 as it had a well-distributed recall, which is what I found most important \n",
    "        while using this model.\n",
    "\n",
    "    3) Class Weight (class_weight):\n",
    "        Using balanced as the class_weight as our data is biased towards the 'No' results with over 90% of the data being labeled 'No'.\n",
    "\n",
    "    4) Evaluation Metrics:\n",
    "        For the metrics used I focused on maintaing a relatively high accuracy while maintaining the 'Yes' recall as high as possible.\n",
    "\n",
    "    5) Hyperparameter Selection Criteria:\n",
    "        I used recall as the scoring metric during GridSearchCV to guide hyperparameter selection. This ensured the tuning process prioritized identifying patients likely to be readmitted within 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea228938",
   "metadata": {},
   "source": [
    "# for data cleansing\n",
    "    1) importing the 2 sheets into pandas dataframes\n",
    "\n",
    "    2) displaying number of null values in the dataset by replacing the '?' with none values to be able to use the isnull function and display the number of null values in each coulmn\n",
    "\n",
    "    3) dropping any coulmn that has null values percentage more than or equal 90%\n",
    "\n",
    "    4) displaying the number of nulls in the remaining coulmns\n",
    "\n",
    "    5) put all the encoding into dictionaries to use in decoding the coulmns\n",
    "\n",
    "    6) decoded the coulmns using one hot encoding\n",
    "\n",
    "    7) putting age in better formate by removing the \"[]\" and \"()\" and then getting the average of the range in the cell\n",
    "\n",
    "    8) truning categorical data into numerical data to help in the training process in the models\n",
    "\n",
    "    9) removing any feature that has correlation less than 0.3 with the target feature\n",
    "\n",
    "    10) detecting and droping every outliers entrie's row by masking and detecting outlier rows one coulmn at a time and removing them by boolean indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f246b",
   "metadata": {},
   "source": [
    "# XGBoost Model Training\n",
    "\n",
    "    1) Prepare the data by removing unnecessary columns         (`patient_nbr`, `encounter_id`, and other readmitted flags), and separate features (X) and target (Y).\n",
    "\n",
    "    2) Split the dataset into training and testing sets with 80% for training and 20% for testing. Use stratification to keep class proportions similar in both sets.\n",
    "\n",
    "    3) Apply SMOTE (Synthetic Minority Over-sampling Technique) to the training data only, to balance the minority class without affecting the test set.\n",
    "\n",
    "    4) Initialize the XGBoost classifier with default parameters.\n",
    "\n",
    "    5) Define a hyperparameter grid to tune parameters like `max_depth`, `n_estimators`, `learning_rate`, `subsample`, and `colsample_bytree`.\n",
    "\n",
    "    6) Use GridSearchCV with 3-fold cross-validation to find the best combination of hyperparameters, optimizing for the F1 score.\n",
    "\n",
    "    7) Train the model on the SMOTE-balanced training data using the best found parameters.\n",
    "\n",
    "    8) Evaluate the final model on the original test data, and print the confusion matrix, classification report, and accuracy score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420a042",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "    1) Feature and Target Selection\n",
    "    We drop irrelevant columns (`patient_nbr`, `encounter_id`, and other readmission labels) and retain the target column `readmitted_<30`.\n",
    "\n",
    "    2) Train-Test Split \n",
    "    The dataset is split into training and testing sets with stratification to maintain class distribution.\n",
    "\n",
    "    3) SMOTE Oversampling \n",
    "    SMOTE (Synthetic Minority Over-sampling Technique) is applied to the training data to balance the class distribution by generating synthetic samples for the minority class.\n",
    "\n",
    "    4) Feature Scaling  \n",
    "    We use `StandardScaler` to normalize feature distributions, which is critical for the performance of Logistic Regression.\n",
    "\n",
    "    5) Model Initialization \n",
    "    A Logistic Regression model is initialized using the `saga` solver (supports both L1 and L2 regularization) and `class_weight='balanced'` to handle remaining imbalance.\n",
    "\n",
    "    6) Hyperparameter Search Space  \n",
    "    We define a grid for regularization strength `C` and regularization type (`l1`, `l2`) to tune using `RandomizedSearchCV`.\n",
    "\n",
    "    7) Hyperparameter Tuning  \n",
    "    `RandomizedSearchCV` is used to find the best hyperparameters via cross-validation, optimizing for the F1 score.\n",
    "\n",
    "    8) Model Training \n",
    "    The model is trained using the SMOTE-resampled and scaled training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a610ecf",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "    1)Data Splitting\n",
    "    The input data is split into features x and labels y. Using train_test_split, the dataset is divided into a training set (60%) and a testing set (40%), with a fixed random state for reproducibility.\n",
    "\n",
    "    2)Feature Scaling\n",
    "    Standardization is applied to ensure that all features contribute equally to the model. The StandardScaler is fit on the training data and then applied to both training and testing sets.\n",
    "\n",
    "    3)Model Initialization and Training\n",
    "    An SVM classifier with a linear kernel (svm.SVC(kernel='linear')) is initialized and trained on the scaled training data. This algorithm is suitable for linearly separable data and supports binary/multiclass classification.\n",
    "\n",
    "    4)Cross-Validation\n",
    "    To evaluate the modelâ€™s robustness, 5-fold cross-validation is conducted on the training data. The average score from the folds gives an estimate of the model's generalization performance.\n",
    "\n",
    "    5)Model Evaluation\n",
    "    After training, the classifier predicts the labels for the scaled test set. The predictions are compared against the true labels using:\n",
    "    classification_report for a detailed breakdown of precision, recall, and F1-score, accuracy_score for the overall accuracy, and manual inspection of the first 10 predictions for a quick qualitative assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef93be",
   "metadata": {},
   "source": [
    "# KNN\n",
    "    1)Data Splitting\n",
    "    The dataset is divided into features x and labels y. A 60-40 train-test split is performed using a fixed random state to ensure reproducibility of results.\n",
    "\n",
    "    2)Feature Scaling\n",
    "    Features are standardized using StandardScaler. Standardization ensures that all features contribute equally by transforming them to have zero mean and unit variance, which is particularly important for distance-based models like KNN.\n",
    "\n",
    "    3)Hyperparameter Grid Definition\n",
    "    A parameter grid is defined for GridSearchCV. It includes various values for:\n",
    "\n",
    "    n_neighbors: Number of nearest neighbors to use for prediction.\n",
    "\n",
    "    weights: 'distance'-based weighting, meaning closer neighbors have more influence.\n",
    "\n",
    "    metric: Euclidean distance used to compute similarity between data points.\n",
    "\n",
    "    4)Model Initialization and Grid Search\n",
    "    A KNeighborsClassifier is instantiated, and a grid search is performed over the specified hyperparameter grid using 5-fold cross-validation. The model is evaluated using accuracy as the scoring metric.\n",
    "\n",
    "    5)Model Selection and Prediction\n",
    "    After the best combination of parameters is found, the optimal KNN model is used to predict the test set labels. This provides the final classification model trained on the best hyperparameters.\n",
    "\n",
    "    6)Model Evaluation\n",
    "    The performance of the best KNN model is assessed using:\n",
    "    Best parameters and cross-validation score from the grid search.\n",
    "    Accuracy score on the unseen test data.\n",
    "    Confusion matrix for evaluating prediction performance per class.\n",
    "    Classification report providing precision, recall, F1-score, and support."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
